{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30b75e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:28:45.692522Z",
     "start_time": "2021-07-19T20:28:45.604502Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FriendQPlayer():\n",
    "    def __init__(self):\n",
    "        self.Q_dimensions = (8,8,2,5,5)\n",
    "        self.Q = self.create_Q()\n",
    "    \n",
    "    def create_Q(self):\n",
    "        return np.ones(self.Q_dimensions)\n",
    "    \n",
    "    def game_matrix(self, agent, opponent, ball_poss):\n",
    "        return self.Q[agent, opponent, ball_poss].T\n",
    "    \n",
    "    def V_function(self, agent, opponent, ball_poss):\n",
    "        return np.max(self.game_matrix(agent, opponent, ball_poss))\n",
    "    \n",
    "    def update_Q(self, agent, opp, ball_poss, reward, agent_prime, opp_prime, ball_poss_prime,\n",
    "                 alpha, gamma\n",
    "                ):\n",
    "\n",
    "        self.Q[int(agent), int(opp), int(ball_poss)] = (1-alpha)*self.Q[int(agent), int(opp), int(ball_poss)]\\\n",
    "        + alpha*((1-gamma)*reward + gamma*self.V_function(int(agent_prime), int(opp_prime), int(ball_poss_prime)))\n",
    "        \n",
    "    def optimal_action(self, agent, opp, ball_poss):\n",
    "        columns = np.amax(self.game_matrix(agent, opp, ball_poss),axis=0)\n",
    "        return np.random.choice(np.flatnonzero(columns == columns.max()))\n",
    "        \n",
    "    def epsilon_greedy_action(self, agent, opp, ball_poss, epsilon):\n",
    "        if epsilon > np.random.random():\n",
    "            return np.random.choice([0,1,2,3,4])\n",
    "            \n",
    "        else:\n",
    "            return self.optimal_action(agent, opp, ball_poss)\n",
    "        \n",
    "class FriendQPlayer():\n",
    "    def __init__(self):\n",
    "        self.Q_dimensions = (8,8,2,5,5)\n",
    "        self.Q = self.create_Q()\n",
    "    \n",
    "    def create_Q(self):\n",
    "        return np.ones(self.Q_dimensions)\n",
    "    \n",
    "    def game_matrix(self, agent, opponent, ball_poss):\n",
    "        return self.Q[agent, opponent, ball_poss].T\n",
    "    \n",
    "    def V_function(self, agent, opponent, ball_poss):\n",
    "        return np.max(self.game_matrix(agent, opponent, ball_poss))\n",
    "    \n",
    "    def update_Q(self, agent, opp, ball_poss, action, opp_action, reward, agent_prime, opp_prime, ball_poss_prime,\n",
    "                 alpha, gamma, done\n",
    "                ):\n",
    "\n",
    "        self.Q[int(agent), int(opp), int(ball_poss), action, opp_action] = (1-alpha)*self.Q[int(agent), int(opp), int(ball_poss), action, opp_action]\\\n",
    "        + alpha*((1-gamma)*reward + gamma*self.V_function(int(agent_prime), int(opp_prime), int(ball_poss_prime)))\n",
    "        \n",
    "    def optimal_action(self, agent, opp, ball_poss):\n",
    "        columns = np.amax(self.game_matrix(agent, opp, ball_poss),axis=0)\n",
    "        return np.random.choice(np.flatnonzero(columns == columns.max()))\n",
    "        \n",
    "    def epsilon_greedy_action(self, agent, opp, ball_poss, epsilon):\n",
    "        if epsilon > np.random.random():\n",
    "            return np.random.choice([0,1,2,3,4])\n",
    "            \n",
    "        else:\n",
    "            return self.optimal_action(agent, opp, ball_poss)\n",
    "        \n",
    "class QLearningPlayer():\n",
    "    def __init__(self):\n",
    "        self.Q_dimensions = (8,8,2,5)\n",
    "        self.Q = self.create_Q()\n",
    "    \n",
    "    def create_Q(self):\n",
    "        return np.ones(self.Q_dimensions)\n",
    "    \n",
    "    def V_function(self, agent, opponent, ball_poss, done):\n",
    "        if not done:\n",
    "            return np.max(self.Q[agent, opponent, ball_poss])\n",
    "        else: return 0\n",
    "    \n",
    "    def update_Q(self, agent, opp, ball_poss, action, opp_action, reward, agent_prime, opp_prime, ball_poss_prime,\n",
    "                 alpha, gamma, done\n",
    "                ):\n",
    "\n",
    "        self.Q[agent, opp, ball_poss, action] = (1-alpha)*self.Q[agent, opp, ball_poss, action]\\\n",
    "        + alpha*((1-gamma)*reward + gamma*self.V_function(agent_prime, opp_prime, ball_poss_prime, done))\n",
    "        \n",
    "    def optimal_action(self, agent, opp, ball_poss):\n",
    "        print(self.Q[agent, opp, ball_poss],\"\\n\")\n",
    "        print(self.Q[agent, opp, ball_poss] == self.Q[agent, opp, ball_poss].max(),\"\\n\")\n",
    "        return np.random.choice(np.flatnonzero(self.Q[agent, opp, ball_poss] == self.Q[agent, opp, ball_poss].max()))\n",
    "        \n",
    "    def epsilon_greedy_action(self, agent, opp, ball_poss, epsilon):\n",
    "        if epsilon > np.random.random():\n",
    "            return np.random.choice([0,1,2,3,4])\n",
    "            \n",
    "        else:\n",
    "            return self.optimal_action(agent, opp, ball_poss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b93ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
