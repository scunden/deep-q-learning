{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19700c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.size = 50000\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        self.sample_threshold = 10000\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        if (len(self.memory) > self.size):\n",
    "            self.memory.pop(0)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def remember(self, observations):\n",
    "        self.memory.append(observations)\n",
    "        self.clear_memory()\n",
    "            \n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def unwrap_batch(self, element, mini_batch):\n",
    "        if element==\"s\":\n",
    "            return [s for s,a,r,done,s_prime in mini_batch]\n",
    "        elif element==\"a\":\n",
    "            return [a for s,a,r,done,s_prime in mini_batch]\n",
    "        elif element==\"r\":\n",
    "            return [r for s,a,r,done,s_prime in mini_batch]\n",
    "        elif element==\"done\":\n",
    "            return [done for s,a,r,done,s_prime in mini_batch]\n",
    "        elif element==\"s_prime\":\n",
    "            return [s_prime for s,a,r,done,s_prime in mini_batch]\n",
    "\n",
    "class LLAgent(object):\n",
    "    def __init__(self, states_dim, actions_dim, alpha, gamma, epsilon_decay_rate, max_episodes, verbose):\n",
    "        self.verbose = verbose\n",
    "        self.D = Memory()\n",
    "        self.states_dim = states_dim\n",
    "        self.actions_dim = actions_dim\n",
    "    \n",
    "        self.nodes = 64\n",
    "        self.alpha = alpha\n",
    "        self.Q = self.nn_model()\n",
    "        self.Q_hat = keras.models.clone_model(self.Q)\n",
    "        self.update_frequency = 500\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.max_epsilon = 1\n",
    "        self.min_epsilon = 0.1\n",
    "        self.epsilon_decay_type = \"exponential\"\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.test_episodes = 100\n",
    "        self.steps = 0\n",
    "        self.reward_tracker_per_episode = []\n",
    "        self.test_reward_tracker_per_episode = []\n",
    "                \n",
    "        \n",
    "    def nn_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,self.states_dim )))\n",
    "        model.add(Dense(self.nodes, activation='relu'))\n",
    "        model.add(Dense(self.nodes, activation='relu'))\n",
    "        model.add(Dense(self.actions_dim, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=\"Adam\")\n",
    "        K.set_value(model.optimizer.learning_rate, self.alpha)\n",
    "        K.set_value(model.optimizer.decay, 0)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_Qhat(self):\n",
    "        self.Q_hat = keras.models.clone_model(self.Q)\n",
    "        self.Q_hat.set_weights(self.Q.get_weights())\n",
    "        \n",
    "    def epsilon_greedy_action(self, state, epsilon):         \n",
    "        if (random.random() <= epsilon):\n",
    "            return random.randint(0, self.actions_dim -1)\n",
    "        else:\n",
    "            return np.argmax(self.Q.predict(np.array([state]))[0])\n",
    "     \n",
    "    def decay_epsilon(self, epsilon):\n",
    "        if epsilon < self.min_epsilon:\n",
    "            return self.min_epsilon\n",
    "        else:\n",
    "            if self.epsilon_decay_type == \"exponential\":\n",
    "                return epsilon*self.epsilon_decay_rate\n",
    "            else:\n",
    "                return epsilon-self.epsilon_decay_rate\n",
    "         \n",
    "    def update_network(self):\n",
    "        \n",
    "        mini_batch = self.D.sample()\n",
    "        states = np.array(self.D.unwrap_batch(\"s\",mini_batch))\n",
    "        actions = self.D.unwrap_batch(\"a\",mini_batch)\n",
    "        rewards = self.D.unwrap_batch(\"r\",mini_batch)\n",
    "        dones = self.D.unwrap_batch(\"done\",mini_batch)\n",
    "        states_prime = np.array(self.D.unwrap_batch(\"s_prime\",mini_batch))\n",
    "        \n",
    "        Y_s  = self.Q.predict(states)\n",
    "        Y_s_prime = self.Q.predict(states_prime)\n",
    "        \n",
    "        Y_s_prime_Q_hat = self.Q_hat.predict(states_prime)\n",
    "        Y = np.zeros((self.D.batch_size , self.actions_dim))\n",
    "        \n",
    "        for i in range(self.D.batch_size):\n",
    "            Y[i,:] = Y_s[i]\n",
    "            if dones[i]:\n",
    "                Y[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                Y[i, actions[i]] = rewards[i] + self.gamma*(Y_s_prime_Q_hat[i, np.argmax(Y_s_prime[i])])\n",
    "                \n",
    "        self.Q.fit(states, Y, batch_size=self.D.batch_size, epochs=1, verbose=False) \n",
    "        \n",
    "    def train_agent(self):\n",
    "        \n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        env = gym.make('LunarLander-v2')\n",
    "        env.seed(SEED)\n",
    "        \n",
    "        epsilon = self.max_epsilon\n",
    "        \n",
    "        for episode in range(self.max_episodes):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            time_step = 0\n",
    "            total_reward = 0.0\n",
    "            epsilon = self.decay_epsilon(epsilon)\n",
    "            while not done:\n",
    "                self.steps += 1\n",
    "                time_step += 1\n",
    "                \n",
    "                s = np.reshape(s, (1,self.states_dim))\n",
    "                action = self.epsilon_greedy_action(s, epsilon)\n",
    "                s_prime, reward, done, info = env.step(action)\n",
    "                s_prime = np.reshape(s_prime, (1,self.states_dim))\n",
    "\n",
    "                total_reward += reward\n",
    "                self.D.remember((s,action,reward,done,s_prime))\n",
    "                \n",
    "                if len(self.D.memory) >= self.D.sample_threshold:\n",
    "                    self.update_network()\n",
    "                    \n",
    "                    if (self.steps%self.update_frequency == 0):\n",
    "                        self.update_Qhat()\n",
    "                    \n",
    "                s = s_prime\n",
    "                \n",
    "            self.reward_tracker_per_episode.append(total_reward)\n",
    "            if self.verbose:\n",
    "                print(\"Episode: {} | Steps: {} | Episode Reward: {} | Epsilon: {}\".format(episode, \n",
    "                                                                                          time_step, \n",
    "                                                                                          total_reward,\n",
    "                                                                                          epsilon\n",
    "                                                                                         ))\n",
    "                \n",
    "    def test_agent(self):\n",
    "        \n",
    "        np.random.seed(SEED)\n",
    "        random.seed(SEED)\n",
    "        env = gym.make('LunarLander-v2')\n",
    "        env.seed(SEED)\n",
    "        \n",
    "        for episode in range(self.test_episodes):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            time_step = 0\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                self.steps += 1\n",
    "                time_step += 1\n",
    "                \n",
    "                s = np.reshape(s, (1,self.states_dim))\n",
    "                action = np.argmax(self.Q.predict(np.array([s]))[0])\n",
    "                s_prime, reward, done, info = env.step(action)\n",
    "                s_prime = np.reshape(s_prime, (1,self.states_dim))\n",
    "\n",
    "                total_reward += reward\n",
    "                s = s_prime\n",
    "                \n",
    "            self.test_reward_tracker_per_episode.append(total_reward)\n",
    "            if self.verbose:\n",
    "                print(\"Episode: {} | Steps: {} | Episode Reward: {}\".format(episode,\n",
    "                                                                            time_step, \n",
    "                                                                            total_reward\n",
    "                                                                           ))\n",
    "                                \n",
    "    def plot_train_rewards(self,plot_type=\"raw\", y_ll=-500, y_ul=300):\n",
    "        \n",
    "        tracker = self.reward_tracker_per_episode\n",
    "        moving_average = []\n",
    "        for i in range(len(tracker)):\n",
    "            moving_average.append(np.mean(tracker[:i+1][-100:]))\n",
    "        if plot_type==\"raw\":\n",
    "            toplot = tracker\n",
    "            y_label = 'Score per Episode'\n",
    "            figure_name = \"Train Raw Score\"\n",
    "        else:\n",
    "            toplot = moving_average\n",
    "            y_label = 'Average Score over Last 100 Episodes'\n",
    "            figure_name=\"Train Avg Score\"\n",
    "\n",
    "\n",
    "        plt.plot([x+1 for x in range(len(toplot))],toplot,\n",
    "                 label=\"γ: {} | α: {} | Decay: {}\".format(self.gamma,self.alpha,self.epsilon_decay_rate))\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(y_label)\n",
    "        plt.axhline(y=200, color='r')\n",
    "        plt.ylim(y_ll, y_ul)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig('{}.png'.format(figure_name))\n",
    "        print(\"Figure Saved\")\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_test_scores(self):\n",
    "        tracker = self.test_reward_tracker_per_episode\n",
    "\n",
    "        moving_average = []\n",
    "        for i in range(len(tracker)):\n",
    "            moving_average.append(np.mean(tracker[:i+1][-100:]))\n",
    "\n",
    "        plt.plot([x+1 for x in range(len(tracker))],tracker,\n",
    "                 label=\"Raw Score\".format(self.gamma,self.alpha,self.epsilon_decay_rate))\n",
    "        plt.plot([x+1 for x in range(len(moving_average))],moving_average,\n",
    "                 label=\"Moving Average\".format(self.gamma,self.alpha,self.epsilon_decay_rate))\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Score')\n",
    "        plt.axhline(y=200, color='r')\n",
    "        plt.ylim(-100, 400)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig('{}.png'.format(\"Test Scores\"))\n",
    "        print(\"Figure Saved\")\n",
    "        plt.close()\n",
    "\n",
    "class HyperparameterTuning(object):\n",
    "    def __init__(self, gamma_ls, alpha_ls, decay_ls, tuning_episodes):\n",
    "        self.starting_gamma = 0.99\n",
    "        self.starting_alpha = 0.00025\n",
    "        self.starting_epsilon_decay_rate = 0.995\n",
    "        \n",
    "        self.optimal_gamma = 0.99\n",
    "        self.optimal_alpha = 0.001\n",
    "        self.optimal_epsilon_decay_rate = 0.99\n",
    "        \n",
    "        self.gamma_ls = gamma_ls\n",
    "        self.alpha_ls = alpha_ls\n",
    "        self.decay_ls = decay_ls\n",
    "        \n",
    "        self.tuning_episodes = tuning_episodes\n",
    "\n",
    "    def tune_gamma(self):\n",
    "        tracker = {}\n",
    "        for gamma in self.gamma_ls:\n",
    "            print(\"Tuning Gamma {}\".format(gamma))\n",
    "            ll = LLAgent(states_dim=8, \n",
    "                         actions_dim=4, \n",
    "                         alpha=self.starting_alpha, \n",
    "                         gamma=gamma, \n",
    "                         epsilon_decay_rate=self.starting_epsilon_decay_rate, \n",
    "                         max_episodes=self.tuning_episodes,\n",
    "                         verbose=False\n",
    "                        )\n",
    "            ll.train_agent()\n",
    "            tracker[gamma, self.starting_alpha, self.starting_epsilon_decay_rate] = ll.reward_tracker_per_episode\n",
    "\n",
    "        return tracker\n",
    "\n",
    "    def tune_alpha(self):\n",
    "        tracker = {}\n",
    "        for alpha in self.alpha_ls:\n",
    "            print(\"Tuning Alpha {}\".format(alpha))\n",
    "            ll = LLAgent(states_dim=8, \n",
    "                         actions_dim=4, \n",
    "                         alpha=alpha, \n",
    "                         gamma=self.optimal_gamma, \n",
    "                         epsilon_decay_rate=self.starting_epsilon_decay_rate, \n",
    "                         max_episodes=self.tuning_episodes,\n",
    "                         verbose=False\n",
    "                        )\n",
    "            ll.train_agent()\n",
    "            tracker[self.optimal_gamma, alpha, self.starting_epsilon_decay_rate] = ll.reward_tracker_per_episode\n",
    "\n",
    "        return tracker\n",
    "\n",
    "    def tune_decay(self):\n",
    "        tracker = {}\n",
    "        for epsilon_decay_rate in self.decay_ls:\n",
    "            print(\"Tuning Decay {}\".format(epsilon_decay_rate))\n",
    "            ll = LLAgent(states_dim=8, \n",
    "                         actions_dim=4, \n",
    "                         alpha=self.optimal_alpha, \n",
    "                         gamma=self.optimal_gamma, \n",
    "                         epsilon_decay_rate=epsilon_decay_rate, \n",
    "                         max_episodes=self.tuning_episodes,\n",
    "                         verbose=False\n",
    "                        )\n",
    "            ll.train_agent()\n",
    "            tracker[self.optimal_gamma, self.optimal_alpha, epsilon_decay_rate] = ll.reward_tracker_per_episode\n",
    "\n",
    "        return tracker\n",
    "\n",
    "    def plot_tuning(self, tracker, figure_name):\n",
    "\n",
    "        combination_tracker = tracker.copy()\n",
    "        gammas = list(set([x[0] for x in combination_tracker.keys()]))\n",
    "\n",
    "        alphas = list(set([x[1] for x in combination_tracker.keys()]))\n",
    "        epsilons = list(set([x[2] for x in combination_tracker.keys()]))\n",
    "        for gamma in gammas:\n",
    "            for alpha in alphas:\n",
    "                for epsilon in epsilons:\n",
    "\n",
    "                    score_tracker = combination_tracker[gamma,alpha,epsilon]\n",
    "                    moving_average = []\n",
    "                    for i in range(len(score_tracker)):\n",
    "                        moving_average.append(np.mean(score_tracker[:i+1][-100:]))\n",
    "\n",
    "\n",
    "                    plt.plot([x+1 for x in range(len(moving_average))],moving_average,\n",
    "                             label=\"γ: {} | α: {} | Decay: {}\".format(gamma,alpha,epsilon))\n",
    "                    plt.xlabel('Episode')\n",
    "                    plt.ylabel('Average Score over Last 100 Episodes')\n",
    "                    plt.ylim(-300, 300)\n",
    "                    \n",
    "                    \n",
    "\n",
    "        plt.legend()\n",
    "        plt.savefig('{}.png'.format(figure_name))\n",
    "        print(\"Figure Saved\")\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24579bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNE = False\n",
    "if tune:\n",
    "    tuner = HyperparameterTuning(gamma_ls=[0.9,0.99,0.999], \n",
    "                         alpha_ls=[0.01,0.001,0.00025], \n",
    "                         decay_ls=[0.9, 0.99, 0.995], \n",
    "                         tuning_episodes=350)\n",
    "\n",
    "    gamma_tracker = tuner.tune_gamma()\n",
    "    tuner.plot_tuning(gamma_tracker,\"Gamma_Tuning\")\n",
    "\n",
    "    alpha_tracker = tuner.tune_alpha()\n",
    "    tuner.plot_tuning(alpha_tracker,\"Alpha_Tuning\")\n",
    "\n",
    "    decay_tracker = tuner.tune_decay()\n",
    "    tuner.plot_tuning(decay_tracker,\"Decay_Tuning\")\n",
    "\n",
    "else:\n",
    "    ll = LLAgent(states_dim=8, \n",
    "         actions_dim=4, \n",
    "         alpha=0.001, \n",
    "         gamma=0.99, \n",
    "         epsilon_decay_rate=0.99, \n",
    "         max_episodes=500,\n",
    "         verbose=True\n",
    "        )\n",
    "    ll.train_agent()\n",
    "    ll.plot_train_rewards(plot_type=\"raw\")\n",
    "    ll.plot_train_rewards(plot_type=\"avg\")\n",
    "\n",
    "    ll.test_agent()\n",
    "    ll.plot_test_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
